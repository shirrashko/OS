shirashko
209144013
EX: 1

FILES:
memory_latency.cpp -- Main program source file that measures memory read latency.
'README' -- This file containing the assignment description and answers to the questions.
'Makefile' -- Makefile for compiling the program.
'results.png' -- Graphical representation of the assignment 2 results obtained from the memory latency measurements.
'lscpu.png' -- Screenshot of the lscpu command output showing CPU details.
'page_size.png' -- Screenshot of the getconf command output showing the system page size.

ANSWERS:
Assignment 1: Understanding WhatIDo - System Call Tracing with strace
WhatIDo program is designed to be executed with exactly one command-line argument. The program performs a series of file
operations and exits successfully. It follows these specific steps:
1. Initialization and Error Handling:
- Argument Check: Immediately upon execution, WhatIDo checks whether exactly one command-line argument has been provided.
If the argument count does not meet this requirement, the program outputs an error message to stderr indicating that a
single argument is expected and terminates with a status code of 0, indicating a normal exit without proceeding to the
main functionalities due to the incorrect number of input arguments.
- Library Setup: If the argument check passes, the program proceeds to load necessary libraries and configurations,
preparing the environment for file operations.
2. File Creation and Manipulation: The program creates a directory named "Welcome", and within it creates three files:
"Welcome", "To", and "OS-2024". It then writes specific messages into these files:
- "Welcome": Contains a reminder to read course guidelines.
- "To": Contains advice to start exercises early.
- "OS-2024": Wishes the user good luck.
3. Cleanup and Exit: After the content writing, the program deletes all created files and the directory, leaving no
trace on the file system. It then exits with a status of 0, signaling successful execution without any errors.

Assignment 2: Measuring Memory Access Latency Across Cache Levels and RAM - Explanation of the Results
In this part of the assignment we aimed to measure memory access latencies across different levels of caches (L1, L2, L3)
and RAM, to understand how data size influences access times relative to cache size. We used a structured approach by
varying the size of data arrays and recording the latency for both random and sequential access patterns.
The measurements were conducted on an Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz on the CSE computers.

The results, visualized in results.png, illustrate the latency (in nanoseconds) as a function of array size (in bytes)
for two test scenarios: sequential and random access. The array size axis is in log scale for better representation.

The program was designed to systematically increase array size from a few bytes to many bytes, thereby crossing the
thresholds of L1, L2, and L3 caches.
Results Observed:
- Cache Efficiency: As expected, data that fit within the L1 and L2 caches were accessed much faster than data that
required retrieval from the L3 cache or system RAM. This demonstrates the effectiveness of CPU cache layers in reducing
data access latency.
- Impact of Cache Boundaries: Significant increases in latency were observed as data sizes exceeded the cache sizes,
with the most dramatic spikes occurring at the transition from L3 cache to RAM.
- Access Patterns: The random access pattern generally resulted in higher latency than sequential access across all data
sizes. This is due to the inefficiency of random accesses in utilizing the prefetching capabilities of modern CPUs,
which are optimized for predictable sequential data streams.
- Practical Implications: For applications involving large datasets, understanding and optimizing for cache usage can
lead to significant reductions in data access times.

Bonus:
As we approach the size of the L3 cache, the latency increases due to more frequent cache misses. This increase is
gradual because not all memory accesses result in an L3 cache miss. At this stage, the page table can still fit within
the L3 cache, so each memory access typically involves one L3 lookup and potentially one RAM access if the data is not
found in the L3 cache. However, once we exceed the page table eviction threshold, entries in the page table begin to
get evicted from the L3 cache. This leads to an additional RAM access for each memory operation. In this scenario, the
CPU must first fetch the page table entry from RAM before retrieving the actual data, effectively doubling the number
of RAM accesses required. As a result, this causes a significant increase in latency.