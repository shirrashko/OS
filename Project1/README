shirashko, login2
209144013
EX: 1

FILES:
memory_latency.cpp -- Main program source file that measures memory read latency.
'README' -- This file containing the assignment description and instructions.
'Makefile' -- Makefile for compiling the program.
'results.png' -- Graphical representation of the results obtained from the memory latency measurements.
'lscpu.png' -- Screenshot of the lscpu command output showing CPU details.

ANSWERS:

# Assignment 1: Understanding WhatIDo - System Call Tracing with strace
WhatIDo program is designed to be executed with exactly one command-line argument. The program performs a series of file operations and exits successfully. It follows these specific steps:
1. Initialization and Error Handling:
- Argument Check: Immediately upon execution, WhatIDo checks whether exactly one command-line argument has been provided. If the argument count does not meet this requirement, the program outputs an error message to stderr indicating that a single argument is expected and terminates with a status code of 0, indicating a normal exit without proceeding to the main functionalities due to the incorrect number of input arguments.
- Library Setup: If the argument check passes, the program proceeds to load necessary libraries and configurations, preparing the environment for file operations.
2. File Creation and Manipulation: The program creates a directory named "Welcome", and within it creates three files: "Welcome", "To", and "OS-2024".
It then writes specific messages into these files:
- "Welcome": Contains a reminder or greeting and a suggestion to read course guidelines.
- "To": Contains advice to start exercises early.
- "OS-2024": Wishes the user good luck.
3. Cleanup and Exit: After the content writing, the program deletes all created files and the directory, leaving no trace on the file system. It then exits with a status of 0, signaling successful execution without any errors.

# Assignment 2: Measuring Memory Access Latency Across Cache Levels and RAM - Explanation of the Results
This part of the assignment was aimed at measuring memory access latencies across different levels of cache (L1, L2, L3) and RAM, to understand how data size influences access times relative to cache size. We used a structured approach by varying the size of data arrays and recording the latency for both random and sequential access patterns.
The measurements were conducted on an Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz on the CSE computers (within a Debian 11 virtual machine). The program was designed to systematically increase array size from a few bytes to many bytes, thereby crossing the thresholds of L1, L2, and L3 caches.
Results Observed:
- Cache Efficiency: As expected, data that fit within the L1 and L2 caches were accessed much faster than data that required retrieval from the L3 cache or system RAM. This demonstrates the effectiveness of CPU cache layers in reducing data access latency.
- Impact of Cache Boundaries: Significant increases in latency were observed as data sizes exceeded the cache sizes, with the most dramatic spikes occurring at the transition from L3 cache to RAM.
- Access Patterns: The random access pattern generally resulted in higher latency than sequential access across all data sizes. This is due to the inefficiency of random accesses in utilizing the prefetching capabilities of modern CPUs, which are optimized for predictable sequential data streams.
Discussion:
- Random vs. Sequential Access: The results clearly illustrated that sequential access benefits immensely from prefetching techniques, reducing latency significantly compared to random access. This underscores the importance of data-oriented design in software development, where structuring data to leverage cache prefetching can lead to substantial performance gains.
- Latency Across Cache Levels: The experiment confirmed theoretical expectations about cache memory performance. Latency remained low and relatively stable as long as data accesses were confined to L1 and L2 caches but increased notably beyond the L3 cache capacity.
- Practical Implications: For applications involving large datasets, understanding and optimizing for cache usage can lead to significant reductions in data access times.